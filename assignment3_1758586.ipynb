{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dx2Ev9EfZyD"
      },
      "source": [
        "### **INSTALLATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV8I3_-Xc6-L",
        "outputId": "3bd3820c-b7bf-4b50-cce0-5c325f9a6c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMpBokCMfenQ",
        "outputId": "4db24bc7-ff28-44c6-e4ec-a873836da24f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KouCMdAfn8d",
        "outputId": "fea72b56-7f81-4bc0-ce15-81a5bbab9779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxGC_G0PfrNJ",
        "outputId": "01caa057-9dc7-4bf1-ac28-258176e6a3f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.0.53)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ntGT19efupH",
        "outputId": "04dda527-1950-4c66-dbcd-a0e0df624f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.10/dist-packages (6.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata) (3.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install importlib_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zjebshmfxqw",
        "outputId": "b49ddb71-c18d-40d7-d31d-16fb34794ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNDsXTc1f40F"
      },
      "source": [
        "### **IMPORT PACKAGES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zWrqMCiafzp9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from nltk.stem.porter import *\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from psutil import virtual_memory\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtB29CDqJTG2"
      },
      "source": [
        "### **CHECK MEMORY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLe7xmHmJUXb",
        "outputId": "ecb57c41-4174-4f40-9dac-52a2010d690e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "# Code from Colab -> was using to check memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Er343TUf_la"
      },
      "source": [
        "### **LOAD THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "b9b6c316736647c2a8e7396c79bebc86",
            "828940dad2784687a3cda85afa29821f",
            "deae1bc82a7e434fbde954a145ca5abc",
            "dc96516bbf8d4751b44bc4d6be6ce16a",
            "d82c1ba7061543dcbfebf379cd635a14",
            "04c2dad4c72f495b846f76eee821bbcb",
            "c2edee220ecd4930aab8295b55f90931",
            "70867c634bb04511b40144faf46e8250",
            "091d7b22dee24bb6978efe5b97c3053d",
            "9c9f0c7de6a84337a907227dc7f63131",
            "56151354e6e548f791dd0473e5759287"
          ]
        },
        "id": "xuwxbxiKf-1c",
        "outputId": "e812dd74-c2bb-4346-cbf9-d92eb150be53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9b6c316736647c2a8e7396c79bebc86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset(\"yelp_review_full\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waPSxu22gHo0"
      },
      "source": [
        "### **LOAD AND SET UP THE TRANSFORMER MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u-BYrozYf96V"
      },
      "outputs": [],
      "source": [
        "dataFrame = pd.DataFrame(dataset[\"train\"])\n",
        "num_labels = len(dataFrame[\"label\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T2d-OVt1ey_Z"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiJqBemjgVva",
        "outputId": "bac6f3ec-dbcd-4a08-f04a-e434a0898a75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "model = torch.hub.load('huggingface/pytorch-transformers', 'model', model_name)\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "last_state_dim = config.hidden_size\n",
        "model.classifier = torch.nn.Linear(last_state_dim, num_labels)\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oli7leuvuj87"
      },
      "source": [
        "### **DATA SPLITTING FOR TRANSFORMER MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9ALZsOoQy0XG"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BKH4gzGzOuTj"
      },
      "outputs": [],
      "source": [
        "chosen = range(100)\n",
        "train_dataset = train_dataset.select(chosen)\n",
        "test_dataset = test_dataset.select(chosen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1V75tU2Vy9h8"
      },
      "outputs": [],
      "source": [
        "ratio = 0.8\n",
        "size = len(train_dataset)\n",
        "train_size = int(ratio * size)\n",
        "val_size = size - train_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fai72HJAzKNK"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xLHJt08qv7tE"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNjrnpWWgcJt"
      },
      "source": [
        "### **PREPROCESSING FOR TRANSFORMER MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SyqldxmIgXXg"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OLlNt2ngfAN",
        "outputId": "5e9ae34d-3ffe-4b91-a007-ca05adc03ad4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BmVv4Upggggm"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    \n",
        "    stemmer = PorterStemmer()\n",
        "    stop = set(stopwords.words('english'))\n",
        "\n",
        "    index = 0\n",
        "    for sentence in text:\n",
        "      tokens = sentence.split()\n",
        "      stemmed_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop]\n",
        "      new_sentence = ' '.join(stemmed_tokens)\n",
        "      text[index] = new_sentence\n",
        "      index += 1\n",
        "\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    \n",
        "    return inputs['input_ids'], inputs['attention_mask']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N_KbS0AIcdj"
      },
      "source": [
        "### **TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CQN_vpKwvzvz"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, epochs):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  train_predicts = []\n",
        "  train_targets = []\n",
        "\n",
        "  val_predicts = []\n",
        "  val_targets = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      batch_idx = 0\n",
        "      for batch in train_loader:\n",
        "\n",
        "        label = batch['label']\n",
        "        text = batch['text']\n",
        "\n",
        "        input_ids, attention_mask = preprocess(text)\n",
        "        input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_states = output.last_hidden_state\n",
        "        final_feature_map = last_hidden_states[:,-1,:]\n",
        "        logits = model.classifier(final_feature_map)\n",
        "\n",
        "        loss = criterion(logits, label)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_idx += 1\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "        label = label.detach().cpu().numpy()\n",
        "        predictions = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "\n",
        "        train_predicts.extend(predictions)\n",
        "        train_targets.extend(label)\n",
        "\n",
        "        accuracy = accuracy_score(train_targets, train_predicts) * 100\n",
        "\n",
        "        print(\"For Batch Number {bi} on Epoch Number {e} there is a Training Loss of {l} with a training accuracy of {a:.4f}%\".format(bi=batch_idx, e=epoch, l=loss, a=accuracy))\n",
        "         \n",
        "      model.eval()\n",
        "\n",
        "      with torch.no_grad(): \n",
        "        for batch in val_loader:\n",
        "          \n",
        "          label = batch['label']\n",
        "          text = batch['text']\n",
        "\n",
        "          input_ids, attention_mask = preprocess(text)\n",
        "          input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n",
        "\n",
        "          output = model(input_ids, attention_mask=attention_mask)\n",
        "          last_hidden_states = output.last_hidden_state\n",
        "          final_feature_map = last_hidden_states[:,-1,:]\n",
        "          logits = model.classifier(final_feature_map)\n",
        "\n",
        "          loss = criterion(logits, label)\n",
        "\n",
        "          predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "          label = label.detach().cpu().numpy()\n",
        "          predictions = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "\n",
        "          val_predicts.extend(predictions)\n",
        "          val_targets.extend(label)\n",
        "\n",
        "          accuracy = accuracy_score(val_targets, val_predicts) * 100\n",
        "\n",
        "          print(\"On Epoch Number {e} there is a Validation Loss of {l} and a validation accuracy of {a:.4f}%\".format(e=epoch, l=loss, a=accuracy))\n",
        "\n",
        "  train_accuracy = accuracy_score(train_targets, train_predicts) *  100\n",
        "  train_precision = precision_score(train_targets, train_predicts, average='weighted', zero_division=1.0) * 100\n",
        "  train_f1 = f1_score(train_targets, train_predicts, average='weighted') * 100\n",
        "  train_recall = recall_score(train_targets, train_predicts, average='macro') * 100\n",
        "\n",
        "  val_accuracy = accuracy_score(val_targets, val_predicts) * 100\n",
        "  val_precision = precision_score(val_targets, val_predicts, average='weighted', zero_division=1.0) * 100\n",
        "  val_f1 = f1_score(val_targets, val_predicts, average='weighted') * 100\n",
        "  val_recall = recall_score(val_targets, val_predicts, average='macro') * 100\n",
        "\n",
        "  print(\"TRAINING ACCURACY: {:.4f}%\".format(train_accuracy))\n",
        "  print(\"TRAINING PRECISION: {:.4f}%\".format(train_precision))\n",
        "  print(\"TRAINING F1: {:.4f}%\".format(train_f1))\n",
        "  print(\"TRAINING RECALL: {:.4f}%\".format(train_recall))\n",
        "\n",
        "  print(\"VALIDATION ACCURACY: {:.4f}%\".format(val_accuracy))\n",
        "  print(\"VALIDATION PRECISION: {:.4f}%\".format(val_precision))\n",
        "  print(\"VALIDATION F1: {:.4f}%\".format(val_f1))\n",
        "  print(\"VALIDATION RECALL: {:.4f}%\".format(val_recall))\n",
        "\n",
        "  print(\"..........................\")\n",
        "\n",
        "  print(\"Training is complete now.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "k8gZwa3Qf0N0"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku7iUnUPH6Mm",
        "outputId": "7a79ac99-9caf-422c-a2e6-ff07b5038609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Batch Number 1 on Epoch Number 0 there is a Training Loss of 1.6156675815582275 with a training accuracy of 18.7500%\n",
            "For Batch Number 2 on Epoch Number 0 there is a Training Loss of 1.6775776147842407 with a training accuracy of 25.0000%\n",
            "For Batch Number 3 on Epoch Number 0 there is a Training Loss of 2.0537309646606445 with a training accuracy of 18.7500%\n",
            "For Batch Number 4 on Epoch Number 0 there is a Training Loss of 1.6675026416778564 with a training accuracy of 20.3125%\n",
            "For Batch Number 5 on Epoch Number 0 there is a Training Loss of 1.6385350227355957 with a training accuracy of 21.2500%\n",
            "On Epoch Number 0 there is a Validation Loss of 1.541701316833496 and a validation accuracy of 37.5000%\n",
            "On Epoch Number 0 there is a Validation Loss of 1.3010544776916504 and a validation accuracy of 40.0000%\n",
            "For Batch Number 1 on Epoch Number 1 there is a Training Loss of 1.3750563859939575 with a training accuracy of 25.0000%\n",
            "For Batch Number 2 on Epoch Number 1 there is a Training Loss of 1.413886308670044 with a training accuracy of 27.6786%\n",
            "For Batch Number 3 on Epoch Number 1 there is a Training Loss of 1.6765159368515015 with a training accuracy of 27.3438%\n",
            "For Batch Number 4 on Epoch Number 1 there is a Training Loss of 1.477587103843689 with a training accuracy of 28.4722%\n",
            "For Batch Number 5 on Epoch Number 1 there is a Training Loss of 1.6223560571670532 with a training accuracy of 30.0000%\n",
            "On Epoch Number 1 there is a Validation Loss of 1.594925045967102 and a validation accuracy of 30.5556%\n",
            "On Epoch Number 1 there is a Validation Loss of 0.8237764239311218 and a validation accuracy of 35.0000%\n",
            "For Batch Number 1 on Epoch Number 2 there is a Training Loss of 1.4407224655151367 with a training accuracy of 30.6818%\n",
            "For Batch Number 2 on Epoch Number 2 there is a Training Loss of 1.368966817855835 with a training accuracy of 31.2500%\n",
            "For Batch Number 3 on Epoch Number 2 there is a Training Loss of 1.1681591272354126 with a training accuracy of 32.2115%\n",
            "For Batch Number 4 on Epoch Number 2 there is a Training Loss of 1.4446697235107422 with a training accuracy of 33.9286%\n",
            "For Batch Number 5 on Epoch Number 2 there is a Training Loss of 1.3575953245162964 with a training accuracy of 35.8333%\n",
            "On Epoch Number 2 there is a Validation Loss of 1.353869915008545 and a validation accuracy of 35.7143%\n",
            "On Epoch Number 2 there is a Validation Loss of 1.0957989692687988 and a validation accuracy of 38.3333%\n",
            "For Batch Number 1 on Epoch Number 3 there is a Training Loss of 1.3088581562042236 with a training accuracy of 35.5469%\n",
            "For Batch Number 2 on Epoch Number 3 there is a Training Loss of 1.4647310972213745 with a training accuracy of 35.2941%\n",
            "For Batch Number 3 on Epoch Number 3 there is a Training Loss of 1.544828176498413 with a training accuracy of 35.7639%\n",
            "For Batch Number 4 on Epoch Number 3 there is a Training Loss of 0.9133748412132263 with a training accuracy of 37.1711%\n",
            "For Batch Number 5 on Epoch Number 3 there is a Training Loss of 1.0634613037109375 with a training accuracy of 37.5000%\n",
            "On Epoch Number 3 there is a Validation Loss of 1.5484580993652344 and a validation accuracy of 35.5263%\n",
            "On Epoch Number 3 there is a Validation Loss of 0.8441976308822632 and a validation accuracy of 38.7500%\n",
            "For Batch Number 1 on Epoch Number 4 there is a Training Loss of 0.989963948726654 with a training accuracy of 38.3929%\n",
            "For Batch Number 2 on Epoch Number 4 there is a Training Loss of 1.1937246322631836 with a training accuracy of 39.2045%\n",
            "For Batch Number 3 on Epoch Number 4 there is a Training Loss of 1.135764479637146 with a training accuracy of 40.7609%\n",
            "For Batch Number 4 on Epoch Number 4 there is a Training Loss of 1.044197678565979 with a training accuracy of 41.1458%\n",
            "For Batch Number 5 on Epoch Number 4 there is a Training Loss of 0.9914770126342773 with a training accuracy of 42.0000%\n",
            "On Epoch Number 4 there is a Validation Loss of 1.7711491584777832 and a validation accuracy of 36.4583%\n",
            "On Epoch Number 4 there is a Validation Loss of 0.7416667938232422 and a validation accuracy of 39.0000%\n",
            "For Batch Number 1 on Epoch Number 5 there is a Training Loss of 0.9928390383720398 with a training accuracy of 43.0288%\n",
            "For Batch Number 2 on Epoch Number 5 there is a Training Loss of 0.9159610271453857 with a training accuracy of 43.9815%\n",
            "For Batch Number 3 on Epoch Number 5 there is a Training Loss of 0.8750882744789124 with a training accuracy of 44.6429%\n",
            "For Batch Number 4 on Epoch Number 5 there is a Training Loss of 0.7503472566604614 with a training accuracy of 45.6897%\n",
            "For Batch Number 5 on Epoch Number 5 there is a Training Loss of 1.015089988708496 with a training accuracy of 46.4583%\n",
            "On Epoch Number 5 there is a Validation Loss of 1.5215258598327637 and a validation accuracy of 38.7931%\n",
            "On Epoch Number 5 there is a Validation Loss of 0.8590145707130432 and a validation accuracy of 40.0000%\n",
            "For Batch Number 1 on Epoch Number 6 there is a Training Loss of 1.0909061431884766 with a training accuracy of 47.1774%\n",
            "For Batch Number 2 on Epoch Number 6 there is a Training Loss of 1.322162389755249 with a training accuracy of 47.6562%\n",
            "For Batch Number 3 on Epoch Number 6 there is a Training Loss of 0.7615206241607666 with a training accuracy of 48.6742%\n",
            "For Batch Number 4 on Epoch Number 6 there is a Training Loss of 0.6919530034065247 with a training accuracy of 49.6324%\n",
            "For Batch Number 5 on Epoch Number 6 there is a Training Loss of 1.1423579454421997 with a training accuracy of 50.0000%\n",
            "On Epoch Number 6 there is a Validation Loss of 1.6134144067764282 and a validation accuracy of 38.9706%\n",
            "On Epoch Number 6 there is a Validation Loss of 0.7052448391914368 and a validation accuracy of 40.7143%\n",
            "For Batch Number 1 on Epoch Number 7 there is a Training Loss of 0.7002121806144714 with a training accuracy of 51.0417%\n",
            "For Batch Number 2 on Epoch Number 7 there is a Training Loss of 0.8181505799293518 with a training accuracy of 51.6892%\n",
            "For Batch Number 3 on Epoch Number 7 there is a Training Loss of 0.9184880256652832 with a training accuracy of 52.1382%\n",
            "For Batch Number 4 on Epoch Number 7 there is a Training Loss of 1.0338705778121948 with a training accuracy of 52.4038%\n",
            "For Batch Number 5 on Epoch Number 7 there is a Training Loss of 0.6173760294914246 with a training accuracy of 53.4375%\n",
            "On Epoch Number 7 there is a Validation Loss of 1.7950876951217651 and a validation accuracy of 39.1026%\n",
            "On Epoch Number 7 there is a Validation Loss of 0.636300802230835 and a validation accuracy of 40.6250%\n",
            "For Batch Number 1 on Epoch Number 8 there is a Training Loss of 0.7489119172096252 with a training accuracy of 54.1159%\n",
            "For Batch Number 2 on Epoch Number 8 there is a Training Loss of 0.9643763303756714 with a training accuracy of 54.4643%\n",
            "For Batch Number 3 on Epoch Number 8 there is a Training Loss of 0.6709465980529785 with a training accuracy of 55.0872%\n",
            "For Batch Number 4 on Epoch Number 8 there is a Training Loss of 0.5809845328330994 with a training accuracy of 55.8239%\n",
            "For Batch Number 5 on Epoch Number 8 there is a Training Loss of 0.9965193867683411 with a training accuracy of 55.8333%\n",
            "On Epoch Number 8 there is a Validation Loss of 1.7826871871948242 and a validation accuracy of 38.6364%\n",
            "On Epoch Number 8 there is a Validation Loss of 1.1567012071609497 and a validation accuracy of 39.4444%\n",
            "For Batch Number 1 on Epoch Number 9 there is a Training Loss of 0.6402576565742493 with a training accuracy of 56.3859%\n",
            "For Batch Number 2 on Epoch Number 9 there is a Training Loss of 0.7762797474861145 with a training accuracy of 56.7819%\n",
            "For Batch Number 3 on Epoch Number 9 there is a Training Loss of 0.8128988146781921 with a training accuracy of 57.1615%\n",
            "For Batch Number 4 on Epoch Number 9 there is a Training Loss of 0.6998298168182373 with a training accuracy of 57.7806%\n",
            "For Batch Number 5 on Epoch Number 9 there is a Training Loss of 0.6272587180137634 with a training accuracy of 58.1250%\n",
            "On Epoch Number 9 there is a Validation Loss of 1.577486515045166 and a validation accuracy of 38.2653%\n",
            "On Epoch Number 9 there is a Validation Loss of 1.2696995735168457 and a validation accuracy of 39.0000%\n",
            "TRAINING ACCURACY: 58.1250%\n",
            "TRAINING PRECISION: 58.3163%\n",
            "TRAINING F1: 57.8938%\n",
            "TRAINING RECALL: 56.4519%\n",
            "VALIDATION ACCURACY: 39.0000%\n",
            "VALIDATION PRECISION: 47.2943%\n",
            "VALIDATION F1: 40.4612%\n",
            "VALIDATION RECALL: 32.7857%\n",
            "..........................\n",
            "Training is complete now.\n"
          ]
        }
      ],
      "source": [
        "train_model(model, optimizer, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMqqJzqBJwdl"
      },
      "source": [
        "### **TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BmV3vE-VTzMD"
      },
      "outputs": [],
      "source": [
        "def test_model(model):\n",
        "\n",
        "  test_predicts = []\n",
        "  test_targets = []\n",
        "\n",
        "  batch_idx = 0\n",
        "  for batch in test_loader:\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      label = batch['label']  \n",
        "      text = batch['text']\n",
        "\n",
        "      input_ids, attention_mask = preprocess(text)\n",
        "      input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n",
        "\n",
        "      output = model(input_ids, attention_mask=attention_mask)\n",
        "      last_hidden_states = output.last_hidden_state\n",
        "      final_feature_map = last_hidden_states[:,-1,:]\n",
        "      logits = model.classifier(final_feature_map)\n",
        "\n",
        "      predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "      label = label.detach().cpu().numpy()\n",
        "      predictions = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "        \n",
        "      test_predicts.extend(predictions)\n",
        "      test_targets.extend(label)\n",
        "\n",
        "      accuracy = accuracy_score(test_targets, test_predicts) * 100\n",
        "\n",
        "      batch_idx += 1\n",
        "\n",
        "      print(\"At Batch Number {bn} the loss is Testing Accuracy is {ta}%\".format(bn=batch_idx, ta=accuracy))\n",
        "\n",
        "  test_accuracy = accuracy_score(test_targets, test_predicts) * 100\n",
        "  test_precision = precision_score(test_targets, test_predicts, average='weighted', zero_division=1.0) * 100\n",
        "  test_f1 = f1_score(test_targets, test_predicts, average='weighted') * 100\n",
        "  test_recall = recall_score(test_targets, test_predicts, average='macro') * 100\n",
        "\n",
        "  print(\"TESTING ACCURACY: {:.4f}%\".format(test_accuracy))\n",
        "  print(\"TESTING PRECISION: {:.4f}%\".format(test_precision))\n",
        "  print(\"TESTING F1: {:.4f}%\".format(test_f1))\n",
        "  print(\"TESTING RECALL: {:.4f}%\".format(test_recall))\n",
        "\n",
        "  print(\"..........................\")\n",
        "\n",
        "  print(\"Testing is complete now.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M-g4fi1VqKe",
        "outputId": "3587ceb7-c144-4601-a2f5-89c5c5b94939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "At Batch Number 1 the loss is Testing Accuracy is 43.75%\n",
            "At Batch Number 2 the loss is Testing Accuracy is 31.25%\n",
            "At Batch Number 3 the loss is Testing Accuracy is 35.41666666666667%\n",
            "At Batch Number 4 the loss is Testing Accuracy is 31.25%\n",
            "At Batch Number 5 the loss is Testing Accuracy is 32.5%\n",
            "At Batch Number 6 the loss is Testing Accuracy is 33.33333333333333%\n",
            "At Batch Number 7 the loss is Testing Accuracy is 33.0%\n",
            "TESTING ACCURACY: 33.0000%\n",
            "TESTING PRECISION: 36.4251%\n",
            "TESTING F1: 32.6460%\n",
            "TESTING RECALL: 32.0401%\n",
            "..........................\n",
            "Testing is complete now.\n"
          ]
        }
      ],
      "source": [
        "test_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1zGzb1qyx7d"
      },
      "source": [
        "### **DATA SPLITTING FOR REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8afQ4VIAsOVK"
      },
      "outputs": [],
      "source": [
        "X_train = [data[\"text\"] for data in train_dataset]\n",
        "Y_train = [data[\"label\"] for data in train_dataset]\n",
        "\n",
        "X_test = [data[\"text\"] for data in test_dataset]\n",
        "Y_test = [data[\"label\"] for data in test_dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJI8Hwfd07bn"
      },
      "source": [
        "### **PREPROCESSING FOR REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "N9SHNPpSYRYI"
      },
      "outputs": [],
      "source": [
        "  vectorizer = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "c8KPbG3RYT11"
      },
      "outputs": [],
      "source": [
        "  X_train_transform = vectorizer.fit_transform(X_train)\n",
        "  X_test_transform = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVx33KATzbUP"
      },
      "source": [
        "### **LOGISTIC REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "MfsY0fOSYcVB",
        "outputId": "001efc52-a258-4591-a610-b194cea72179"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.5, class_weight=&#x27;balanced&#x27;, max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.5, class_weight=&#x27;balanced&#x27;, max_iter=1000)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(C=0.5, class_weight='balanced', max_iter=1000)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LR_model = LogisticRegression(penalty='l2',max_iter=1000, C=0.5, class_weight='balanced')\n",
        "LR_model.fit(X_train_transform, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1KuaZI1xYetv"
      },
      "outputs": [],
      "source": [
        "Y_prediction = LR_model.predict(X_test_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-jIlNIHUpIZ2"
      },
      "outputs": [],
      "source": [
        "reg_accuracy = accuracy_score(Y_test, Y_prediction) * 100\n",
        "reg_precision = precision_score(Y_test, Y_prediction, average='weighted') * 100\n",
        "reg_f1_score = f1_score(Y_test, Y_prediction, average='weighted') * 100\n",
        "reg_recall = recall_score(Y_test, Y_prediction, average='macro') * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG_C98n4m4n9",
        "outputId": "15c7b52b-8434-4bbf-fc47-84840516e53b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy: 35.0000%\n",
            "Logistic Regression Precision: 34.5332%\n",
            "Logistic Regression F1 Score: 33.2330%\n",
            "Logistic Regression Recall: 32.7361%\n"
          ]
        }
      ],
      "source": [
        "print(\"Logistic Regression Accuracy: {:.4f}%\".format(reg_accuracy))\n",
        "print(\"Logistic Regression Precision: {:.4f}%\".format(reg_precision))\n",
        "print(\"Logistic Regression F1 Score: {:.4f}%\".format(reg_f1_score))\n",
        "print(\"Logistic Regression Recall: {:.4f}%\".format(reg_recall))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLlUkx8vOfWI"
      },
      "source": [
        "### **CITATIONS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk40TDcgOvu1"
      },
      "source": [
        "\n",
        "*   Professor's Code\n",
        "*   TA's Code\n",
        "*   \"Pandas.Unique#.” Pandas.Unique - Pandas 2.0.2 Documentation, 2023, pandas.pydata.org/docs/reference/api/pandas.unique.html. \n",
        "*   Process, huggingface.co/docs/datasets/process. \n",
        "*   “Python Torch.Utils.Data.Random_split() Examples.” Python Examples of Torch.Utils.Data.Random_split, www.programcreek.com/python/example/125046/torch.utils.data.random_split. \n",
        "*   “Documentation.” NLTK, 2 Jan. 2023, www.nltk.org/howto/stem.html. \n",
        "*   “Removing Stop Words with NLTK in Python.” GeeksforGeeks, 16 May 2023, www.geeksforgeeks.org/removing-stop-words-nltk-python/. \n",
        "*   “Torch.Argmax¶.” Torch.Argmax - PyTorch 2.0 Documentation, 2023, pytorch.org/docs/stable/generated/torch.argmax.html. \n",
        "*   Admin. “Create Numpy Array from Pytorch Tensor Using Detach().Numpy().” For Machine Learning, 5 May 2023, androidkt.com/create-numpy-array-from-pytorch-tensor-using-detach-numpy/. \n",
        "*  “3.3. Metrics and Scoring: Quantifying the Quality of Predictions.” Scikit, 2007, scikit-learn.org/stable/modules/model_evaluation.html.\n",
        "*   “Sklearn.Feature_extraction.Text.TfidfVectorizer.” Scikit, 2007, scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html. \n",
        "*   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04c2dad4c72f495b846f76eee821bbcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091d7b22dee24bb6978efe5b97c3053d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56151354e6e548f791dd0473e5759287": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70867c634bb04511b40144faf46e8250": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "828940dad2784687a3cda85afa29821f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04c2dad4c72f495b846f76eee821bbcb",
            "placeholder": "​",
            "style": "IPY_MODEL_c2edee220ecd4930aab8295b55f90931",
            "value": "100%"
          }
        },
        "9c9f0c7de6a84337a907227dc7f63131": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9b6c316736647c2a8e7396c79bebc86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_828940dad2784687a3cda85afa29821f",
              "IPY_MODEL_deae1bc82a7e434fbde954a145ca5abc",
              "IPY_MODEL_dc96516bbf8d4751b44bc4d6be6ce16a"
            ],
            "layout": "IPY_MODEL_d82c1ba7061543dcbfebf379cd635a14"
          }
        },
        "c2edee220ecd4930aab8295b55f90931": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d82c1ba7061543dcbfebf379cd635a14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc96516bbf8d4751b44bc4d6be6ce16a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c9f0c7de6a84337a907227dc7f63131",
            "placeholder": "​",
            "style": "IPY_MODEL_56151354e6e548f791dd0473e5759287",
            "value": " 2/2 [00:00&lt;00:00, 65.90it/s]"
          }
        },
        "deae1bc82a7e434fbde954a145ca5abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70867c634bb04511b40144faf46e8250",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_091d7b22dee24bb6978efe5b97c3053d",
            "value": 2
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
