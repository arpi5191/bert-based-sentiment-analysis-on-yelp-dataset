# -*- coding: utf-8 -*-
"""assignment3_1758586.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f8WYmB1TKwYsFr4e1di2cUMXRqiOBBoI

### **INSTALLATIONS**
"""

#!pip install datasets

#!pip install transformers

#!pip install sentencepiece

#!pip install sacremoses

#!pip install importlib_metadata

#!pip install nltk

"""### **IMPORT PACKAGES**"""

import nltk
import torch
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.optim as optim
from nltk.stem.porter import *
import torch.utils.data as data
import torch.nn.functional as F
from datasets import load_dataset
# from psutil import virtual_memory
from nltk.corpus import stopwords
# from gensim.models import Word2Vec
from transformers import AutoConfig
from transformers import AutoTokenizer
from nltk.tokenize import word_tokenize
from sklearn.metrics import accuracy_score
from torch.nn.utils.rnn import pad_sequence
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors

"""### **CHECK MEMORY**"""

# if torch.cuda.is_available():
#     print('GPU available')
# else:
#     print('Not connected to a GPU')
#
# ram_gb = virtual_memory().total / 1e9
# print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))
#
# if ram_gb < 20:
#   print('Not using a high-RAM runtime')
# else:
#   print('You are using a high-RAM runtime!')

"""### **LOAD THE DATA**"""

dataset = load_dataset("yelp_review_full")

"""### **LOAD AND SET UP THE TRANSFORMER MODEL**"""

dataFrame = pd.DataFrame(dataset["train"])

labels = dataFrame["label"].tolist()
num_labels = len(set(labels))

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model_name = "bert-base-uncased"
model = torch.hub.load('huggingface/pytorch-transformers', 'model', model_name)
config = AutoConfig.from_pretrained(model_name)
last_state_dim = config.hidden_size
model.classifier = torch.nn.Linear(last_state_dim, num_labels)
model.to(device)
model.eval()

"""### **PREPROCESSING FOR TRANSFORMER MODEL**"""

tokenizer = AutoTokenizer.from_pretrained(model_name)

nltk.download('stopwords')

def preprocess(text):
  # pdb.set_trace()
#
  tokens = tokenizer.tokenize(text)

  stemmer = PorterStemmer()
  stop = set(stopwords.words('english'))

  stemmed_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop]

  if len(stemmed_tokens) > 512:
      stemmed_tokens = stemmed_tokens[:512]

  inputs = tokenizer.batch_encode_plus(stemmed_tokens, padding=True, truncation=True, return_tensors='pt')

  # inputs = tokenizer(stemmed_tokens, padding=True, truncation=True, return_tensors='pt')

  ids = inputs['input_ids']
  attention = inputs['attention_mask']

  return ids, attention

"""### **DATA SPLITTING FOR TRANSFORMER MODEL**"""

train_dataset = dataset["train"]
test_dataset = dataset["test"]

size = len(train_dataset)
train_size = int(0.8 * size)
val_size = size - train_size

train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False)

"""### **TRAINING**"""

def predict_sentiment(texts):
  input_ids, attention_mask = preprocess(texts)

  with torch.no_grad():
      hidden_states = model(input_ids, attention_mask=attention_mask)
      print(hidden_states)
      last_hidden_states = hidden_states.last_hidden_state
      print(last_hidden_states)
      print(last_hidden_states.shape)
      final_feature_map = last_hidden_states[:,-1,:]

  logits = model.classifier(final_feature_map)
  print(logits.shape)
  probabilities = F.softmax(logits, dim=1)
  # sentiments = torch.argmax(probabilities, dim=1).squeeze()

  return probabilities

import pdb
from torch.nn.utils.rnn import pad_sequence

def train_model(model, optimizer):

  model.train()

  for epoch in range(epochs):
      batch_idx = 1
      for batch in train_loader:
        label = batch['label']

        num_classes = torch.unique(label).size(0)

        text = batch['text']
        text = ''.join(text)

        input_ids, attention_mask = preprocess(text)

        optimizer.zero_grad()

        output = model(input_ids, attention_mask=attention_mask)

        last_hidden_states = output.last_hidden_state

        final_feature_map = last_hidden_states[:,-1,:]

        logits = model.classifier(final_feature_map)

        logits = logits[:label.size(0)]

        loss = F.cross_entropy(logits, label)

        print(loss)
        #
        # print(loss)

        # ids = torch.Tensor(ids)
        # attention = Torch.Tensor(attention)
        #
        # print(ids.shape)
        # print(attention.shape)

        # padded_ids = [F.pad(tensor, (0, 512 - len(tensor)), value=0) for tensor in ids_list]
        # padded_logits = [F.pad(tensor, (0, 512 - len(tensor)), value=0) for tensor in logits_list]



        # output = model(logits.long(), ids.long())


        # labels = torch.Tensor(target)
        # loss = F.cross_entropy(logits, labels)
        loss.backward()
        optimizer.step()

      #   break
      # break

        # ids_list = []
        # logits_list = []
        # for sentence in text:
        #   ids, attention = preprocess(sentence)
        #   ids, attention = ids.to(device), attention.to(device)
        #   ids_list.append(ids.view(-1))
        #   logits = predict_sentiment(ids, attention)
        #   logits_list.append(logits.view(-1))

        # padded_ids = [F.pad(tensor, (0, 512 - len(tensor)), value=0) for tensor in ids_list]
        # padded_logits = [F.pad(tensor, (0, 512 - len(tensor)), value=0) for tensor in logits_list]
        #
        # padded_ids = torch.stack(padded_ids)
        # padded_logits = torch.stack(padded_logits)
        #
        # optimizer.zero_grad()
        #
        # output = model(padded_logits.long(), padded_ids.long())
        #
        #
        # labels = torch.Tensor(target)
        # loss = F.cross_entropy(output.pooler_output, labels)
        # loss.backward()
        # optimizer.step()

epochs = 10
learning_rate = 0.01

# base_model_parameters = model.base_model.parameters()
#
# # Get the parameters of the classifier
# classifier_parameters = model.classifier.parameters()
#
# # Concatenate the parameters
# all_parameters = list(base_model_parameters) + list(classifier_parameters)
#
# # Define the optimizer with all parameters
# optimizer = torch.optim.SGD(all_parameters, lr=learning_rate, weight_decay=0.001)

# print(model.parameters())
#
# print(model.classifier())

optimizer = torch.optim.SGD(model.classifier.parameters(), lr=learning_rate, weight_decay=0.001)

# result = preprocess("Hello welcome back home. I'm glad to see you.")
# print(result)

train_model(model, optimizer)







        # print(output.last_hidden_state)

        # print(output.keys())

        # logits = output.get('logits')

        # print(type(logits))


       # print(type(padded_logits))
        # print(type(padded_ids))

        # max_id_size = max(len(tensor) for tensor in ids_list)
        # max_logits_size = max(len(tensor) for tensor in logits_list)

        # if max_id_size > max_logits_size:
        #   max_size = max_id_size
        # else:
        #   max_size = max_logits_size

        # print(max_id_size)
        # print(max_logits_size)

        # # Convert the padded tensors to a single tensor
        # padded_tensor = pad_sequence(padded_tensors, batch_first=True)

        # # Print the padded tensor
        # print(padded_tensor)

        # optimizer.zero_grad()

        # output = model(inputs, attention)

        # loss = F.cross_entropy(output.last_hidden_state, target)
        # loss.backward()

        # optimizer.step()

        # predicted_sentiments = torch.argmax(output.last_hidden_state, dim=1)

        # train_predicts.extend(predicted_sentiments.tolist())
        # train_targets.extend(target.tolist())

        # train_accuracy = accuracy_score(target, predicted_sentiments)
        # train_precision = precision_score(target, predicted_sentiments)
        # train_recall = recall_score(target, predicted_sentiments)
        # train_f1_score = f1_score(target, predicted_sentiments)

        # print("BATCH NUMBER: {batch_idx}, EPOCH NUMBER: {epoch}".format(batch_idx=batch_idx, epoch=epoch))
        # print("Training Loss: {loss}".format(loss=loss))
        # print("Training Accuracy: {acc}, Training Precision: {prec}, Training Recall: {rec}, Training F1-Score: {f1}".format(acc=train_accuracy,
        #                                                                               prec=train_precision, rec=train_recall, f1=train_f1_score))

        # batch_idx += 1

  #     model.eval()

  #     with torch.no_grad():
  #       for batch in val_loader:
  #         target = batch['label']

  #         raw_text = batch['text']
  #         # text = ''.join(raw_text)

  #         inputs, attention = preprocess(text)
  #         inputs, attention, target = inputs.to(device), attention.to(device), target.to(device)

  #         padded_inputs = pad_sequence(inputs, batch_first=True)
  #         padded_attention = pad_sequence(attention, batch_first=True)

  #         output = model(padded_inputs, padded_attention)

  #         loss = F.cross_entropy(output.last_hidden_state, target)

  #         # predicted_sentiments = torch.argmax(output.last_hidden_state, dim=1)

  #         # val_predicts.extend(predicted_sentiments.tolist())
  #         # val_targets.extend(target.tolist())

  #         # val_accuracy = accuracy_score(target, predicted_sentiments)
  #         # val_precision = precision_score(target, predicted_sentiments)
  #         # val_recall = recall_score(target, predicted_sentiments)
  #         # val_f1_score = f1_score(target, predicted_sentiments)

  #         print("BATCH NUMBER: {batch_idx}, EPOCH NUMBER: {epoch}".format(batch_idx=batch_idx, epoch=epoch))
  #         print("Validation Loss: {loss}".format(loss=loss))
  #         # print("Validation Accuracy: {acc}, Validation Precision: {prec}, Validation Recall: {rec}, Validation F1-Score: {f1}".format(acc=val_accuracy,
  #         #                                                                             prec=val_precision, rec=val_recall, f1=val_f1_score))

  # # final_train_accuracy = accuracy_score(train_targets, train_predicts)
  # # final_train_precision = precision_score(train_targets, train_predicts)
  # # final_train_recall = recall_score(train_targets, train_predicts)
  # # final_train_f1_score = f1_score(train_targets, train_predicts)

  # # final_val_accuracy = accuracy_score(val_targets, val_predicts)
  # # final_val_precision = precision_score(val_targets, val_predicts)
  # # final_val_recall = recall_score(val_targets, val_predicts)
  # # final_val_f1_score = f1_score(val_targets, val_predicts)

  # # print("Final Training Accuracy: {ta}".format(ta=final_train_accuracy))
  # # print("Final Training Precision: {tp}".format(tp=final_train_precision))
  # # print("Final Training Recall: {tr}".format(tr=final_train_recall))
  # # print("Final Training F1-Score: {fs}".format(fs=final_train_f1_score))

  # # print("Final Validation Accuracy: {va}".format(va=final_val_accuracy))
  # # print("Final Validation Precision: {vp}".format(vp=final_val_precision))
  # # print("Final Validation Recall: {vr}".format(vr=final_val_recall))
  # # print("Final Validation F1-Score: {vs}".format(vs=final_val_f1_score))

#   print("..........................")
#
#   print("Training is complete now.")
#


# max_grad_norm = 1.0  # Set the maximum gradient norm
# torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)



# import torch
# from torch.nn.utils.rnn import pad_sequence

# # Example list of tensors
# ids_list = [torch.tensor([1, 2, 3]), torch.tensor([4, 5]), torch.tensor([6, 7, 8, 9])]

# # Find the maximum length among all the tensors
# max_length = max(len(ids) for ids in ids_list)

# # Pad each tensor to the maximum length
# padded_sequence = pad_sequence(ids_list, batch_first=True, padding_value=0)

# # Check the shape of the padded sequence
# print(padded_sequence.shape)

"""### **TESTING**"""

# def test_model(model):

#   test_predicts = []
#   test_targets = []

#   for batch in test_loader:

#     model.eval()

#     with torch.no_grad():

#       target = batch['label']

  #     raw_text = batch['text']
  #     text = ''.join(raw_text)

  #     sentiments, probabilities = predict_sentiment(text)

  #     inputs, attention = preprocess(text)
  #     inputs, attention, target = inputs.to(device), attention.to(device), target.to(device)

  #     padded_inputs = pad_sequence(inputs, batch_first=True)
  #     padded_attention = pad_sequence(attention, batch_first=True)

  #     output = model(padded_inputs, padded_attention)

  #   predicted_sentiments = torch.argmax(output, dim=1)

  #   test_predicts.extend(predicted_sentiments.tolist())
  #   test_targets.extend(target.tolist())

  # final_test_accuracy = accuracy_score(test_targets, test_predicts)
  # final_test_precision = precision_score(test_targets, test_predicts)
  # final_test_recall = recall_score(test_targets, test_predicts)
  # final_test_f1_score = f1_score(test_targets, test_predicts)

  # print("Final Validation Accuracy: {ta}".format(va=final_test_accuracy))
  # print("Final Validation Precision: {tp}".format(vp=final_test_precision))
  # print("Final Validation Recall: {tr}".format(vr=final_test_recall))
  # print("Final Validation F1-Score: {ts}".format(vs=final_test_f1_score))

  # print("..........................")

  # print("Testing is complete now.")

# test_model(model)

"""### **DATA SPLITTING FOR REGRESSION**"""

# size = len(train_dataset)
# train_size = int(0.5 * size)
# val_size = size - train_size

# train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])

# X_train, X_val, y_train, y_val = train_test_split(train_dataset, val_dataset, random_state=1000)

"""### **PREPROCESSING FOR REGRESSION**"""

# def convert_features_to_one_hot(df, feature_name_list):
#   for feature_name in feature_name_list:
#     df = pd.get_dummies(df, columns=[feature_name])
#   return df

# X_train_dataframe = pd.DataFrame(X_train)
# trainXFeatures = X_train_dataframe.select_dtypes(include=['object']).columns.tolist()
# X_train = convert_features_to_one_hot(X_train_dataframe, trainXFeatures)

# X_val_dataframe = pd.DataFrame(X_val)
# valXFeatures = X_val_dataframe.select_dtypes(include=['object']).columns.tolist()
# X_val = convert_features_to_one_hot(X_val_dataframe, valXFeatures)

# Y_train_dataframe = pd.DataFrame(y_train)
# trainYFeatures = Y_train_dataframe.select_dtypes(include=['object']).columns.tolist()
# y_train = convert_features_to_one_hot(Y_train_dataframe, trainYFeatures)

# Y_val_dataframe = pd.DataFrame(y_val)
# valYFeatures = Y_val_dataframe.select_dtypes(include=['object']).columns.tolist()
# y_val = convert_features_to_one_hot(Y_val_dataframe, valYFeatures)

"""### **LOGISTIC REGRESSION**"""

# model = LogisticRegression(multi_class='multinomial', solver='lbfgs')

# model.fit(X_train, y_train)

# y_pred = model.predict(X_test)

# logistic_accuracy = accuracy_score(y_test, y_pred)
# logistic_precision = precision_score(y_test, y_pred)
# logistic_recall = recall_score(y_test, y_pred)
# logistic_f1_score = (y_test, y_pred)

# print("Logistic Regression Accuracy: {la}".format(la=logistic_accuracy))
# print("Logistic Regression Precision: {lp}".format(lp=logistic_precision))
# print("Logistic Regression Recall: {lr}".format(lr=logistic_recall))
# print("Logistic Regression F1-Score: {ls}".format(ls=logistic_f1_score))
